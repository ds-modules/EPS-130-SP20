{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake Occurrence Statistics\n",
    "\n",
    "The statistics of earthquake occurrence is revealed from catalogs of seismicity, which include event time, location and magnitude. We will talk about how earthquakes are located and how magnitudes are estimated separately, but for now it is sufficient to know that this information can be easily acquired. With such catalogs it is possible to compare the seismic activity of different regions, make informed assessments about the frequency of earthquake occurrence, and learn about the fault rupture process. Maps of the earthquakes in catalogs over time reveal the structure of faulting in a region, and provide a framework with which to study the seismotectonics of a region.\n",
    "\n",
    "There are two primary earthquake statistics used by seismologists. They are the Gutenberg-Richter relationship (Gutenberg and Richter, 1949), and the Omori Law (Omori, 1894). \n",
    "\n",
    "Gutenberg and Richter found that when the logarithm of the number of earthquakes is plotted vs. magnitude that the distribution (data) may be described by the line (model), log(N)=A+Bm, where N is the number of earthquakes, m is the magnitude and A (y-intercept) and B (slope) are refered to as the Gutenberg-Richter statistics or coefficients. They found that on a global scale, and subsequently more generally, the B-value or the slope of the Gutenberg-Richter line is approximately equal to -1. Thus for each increase in earthquake magnitude there are approximately 10 times fewer earthquakes. If, for example, there are 100 M3 events in a region per year, then the Gutenberg-Richter relationship generally finds that there would be approximately 10 M4 events and 1 M5 event in each year. For magnitudes larger than M5 there would be fewer than one event per year. Gutenberg-Richter is a very important earthquake statistic because it is used to determine the rates of earthquake occurrence, which is a key step in characterizing earthquake hazards (we will see this in future homework exercises). \n",
    "\n",
    "The Omori Law is used to characterize the rate at which aftershocks occur following a large mainshock event. This statistic is used for comparing the aftershock productivity of different earthquakes and regions, make forecasts of the likelihood of large damaging aftershocks and to distinguish between earthquake faulting and possibly geothermal or volcanic-related seismicity by examining whether the distribution describes a \"mainshock/aftershock\" pattern or is more \"swarm-like\". \n",
    "\n",
    "In this homework you will use python code in this notebook to investigate the Gutenberg-Richter and Omori statistics for the San Francisco Bay Area, as well as develop numerical analysis skills using python. \n",
    "\n",
    "Note: This is not a python class, but the primary programming tool that will be used is python. However, if you know MatLab or have other programing background and would prefer to use it, you are free to use those tools instead. It will be helpful to read sections 9.6 and 9.8 of Lay and Wallace (1995) prior to working on this laboratory for background on the Gutenberg-Richter relation and the Omori Law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m  Failed building wheel for cartopy\u001b[0m\n",
      "\u001b[31mCommand \"/usr/local/opt/python@2/bin/python2.7 -u -c \"import setuptools, tokenize;__file__='/private/var/folders/73/c35f8qv50k1_ps6zt8y5ckqw0000gq/T/pip-install-5dVmrJ/cartopy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/73/c35f8qv50k1_ps6zt8y5ckqw0000gq/T/pip-record-7b3CP9/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/73/c35f8qv50k1_ps6zt8y5ckqw0000gq/T/pip-install-5dVmrJ/cartopy/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cartopy -q\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two geographic points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "    \n",
    "    The first pair can be singular and the second an array\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371.0 * c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDays(c,y,m,d):\n",
    "    '''\n",
    "    Function to count days in the array\n",
    "    '''\n",
    "    days=np.zeros(c)\n",
    "    for i in range(0,c,1):\n",
    "        d0 = datetime.date(y[0], m[0], d[0])\n",
    "        d1 = datetime.date(y[i], m[i], d[i])\n",
    "        delta = d1 - d0\n",
    "        days[i]=delta.days\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAnssCatalog(p):\n",
    "    '''\n",
    "    Function to slice an ANSS catalog loaded as a pandas dataframe and return arrays of info, including days\n",
    "    '''\n",
    "    d=np.array(p)         # load the dataframe into numpy as an array      \n",
    "    year=d[:,0].astype(int)  # define variables from the array\n",
    "    month=d[:,1].astype(int)\n",
    "    day=d[:,2].astype(int)\n",
    "    hour=d[:,3].astype(int)\n",
    "    minute=d[:,4].astype(int)\n",
    "    sec=d[:,5].astype(int)\n",
    "    lat=d[:,6]\n",
    "    lon=d[:,7]\n",
    "    mag=d[:,8]\n",
    "    days = countDays(len(year),year,month,day)\n",
    "    return year,month,day,hour,minute,sec,lat,lon,mag,days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Catalog\n",
    "We have downloaded the Advanced National Seismic System (ANSS) catalog from 1900 to 2018 for you to use (also available here: http://www.quake.geo.berkeley.edu/anss/catalog-search.html), and saved it as a text-file named \"anss_catalog_1900to2018all.txt\". This catalog has all events in the aforementioned time range located within 100 km of UC Berkeley. Columns of this catalog include information about the catalogued earthquakes, including the date and time of each event, its location in latitude, longitude and depth, and the event magnitude.  \n",
    "\n",
    "The following python code reads this catalog file and places the information in arrays for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'anss_catalog_1900to2018all.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-54ebc2300edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# big enough to include Loma Prieta but exclude Geysers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m data= pd.read_csv('anss_catalog_1900to2018all.txt', sep=' ', delimiter=None, header=None,\n\u001b[0;32m----> 6\u001b[0;31m                 names = ['Year','Month','Day','Hour','Min','Sec','Lat','Lon','Mag'])\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadAnssCatalog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/py37/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'anss_catalog_1900to2018all.txt' does not exist"
     ]
    }
   ],
   "source": [
    "#Read data and create data arrays\n",
    "\n",
    "# This catalog is a M0+ search centered at Berkeley radius=100. \n",
    "# big enough to include Loma Prieta but exclude Geysers\n",
    "data= pd.read_csv('anss_catalog_1900to2018all.txt', sep=' ', delimiter=None, header=None,\n",
    "                names = ['Year','Month','Day','Hour','Min','Sec','Lat','Lon','Mag'])\n",
    "\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Explore the raw catalog (10 pts)\n",
    "readAnssCatalog(data)\n",
    "print (year)\n",
    "print (month)\n",
    "print (hour)\n",
    "print (mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the number of events, the number of days from the first event, the minimum magnitude, and the maximum magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevt=len(year)\n",
    "print (nevt)\n",
    "firsteventday = (day[0])\n",
    "print (firsteventday)\n",
    "daysfromfirstevent = (day - day[0])\n",
    "minimum_magnitude=min(mag)\n",
    "print (minimum_magnitude)\n",
    "maximum_magnitude = max(mag)\n",
    "print (maximum_magnitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the catalog time series\n",
    "Make an x-y plot showing the magnitude of the earthquake on the y-axis and the time of the event on the x-axis. For this it is useful to have already determined the days since the beginning of the catalog. The plot will show that the catalog is not uniform due to the fact that over time as more seismic recording stations were installed more earthquakes could be detected and properly located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(year,mag) # ******\n",
    "ax.set(xlabel='year', ylabel='magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "fig.savefig(\"hw1_ex1_fig1.png\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the catalog in map view\n",
    "#Familiarize yourself with the code example below as there will be additional exercises requiring the plotting of maps.\n",
    "\n",
    "#- Describe the seismicity and any patterns that you see.\n",
    "#In this raw event catalog of a 100 year period, One ppattern I notice is that there seems to be a negative correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Try adding fault traces.\n",
    "fig, ax = plt.subplots()\n",
    "fault_traces = max(mag)\n",
    "ax.plot(year,mag,fault_traces)\n",
    "ax.set(xlabel='year', ylabel='magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- How well does the seismicity show the region's major faults?\n",
    "\n",
    "#I can't tell if the regions major fault is the concentrated region on the first graph, i know that my graph directly above is not accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map\n",
    "\n",
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "ydim=10      #height of plot\n",
    "xdim=ydim*(haversine_np(lon0,lat0,lon1,lat0)/haversine_np(lon0,lat0,lon0,lat1)) #scale width\n",
    "\n",
    "###\n",
    "plt.figure(figsize=(ydim,xdim))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='longitude', ylabel='Latitude',\n",
    "       title='Raw Catalog')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES,alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS)\n",
    "ax.add_feature(cfeature.STATES.with_scale('10m'))\n",
    "\n",
    "# Plot events as open circles with size and color proportional to event magnitude\n",
    "indx=np.argsort(mag)   #determine sort index #Sort Descending to plot largest events on top\n",
    "x=lon[indx]            #apply sort index\n",
    "y=lat[indx]\n",
    "z=np.exp(mag[indx])    #exponent to scale size\n",
    "c = plt.cm.plasma(z/max(z))\n",
    "plt.scatter(x,y, s=c, facecolors='none', edgecolors=c, marker='o', linewidth=2, alpha=0.5) # *****\n",
    "\n",
    "\n",
    "# Add Berkeley, CA as a red square with size proportional to event magnitude\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=10) # *****\n",
    "\n",
    "#Save the plot by calling plt.savefig() BEFORE plt.show()\n",
    "plt.savefig('hw1_ex2_seismap_raw.pdf')\n",
    "plt.savefig('hw1_ex2_seismap_raw.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Compute the Gutenberg-Richter statitistics (30 pts)\n",
    "\n",
    "Follow the steps below to compute the Gutenberg Richter statistics for the raw catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine and plot the Gutenberg-Richter Distribution\n",
    "First, define a range of magnitudes to bin the data. You can use a range of magnitude, m from 0.0 to 6.9 in increments of 0.1 magnitude unit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.arange(0, 6.9, 0.1) # ***** define a range of mag bins with width 0.1 magnitude per bin\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, count the number of events above a given magnitude. That is count the number of events above and equal to magnitude 0.0, then above and equal to 0.1, and so forth all the way to the maximum magnitude. You can do this by placing the code for vectorized counting of array elements passing a logical test (numpy.count_nonzero()) inside a for loop over the incremental magnitudes, m. We are interested in the annual rate of the events so you will need to divide by the total number of years the catalog spans. N is the log of of the number of events per year, so take the log base 10 (numpy.log10) of the annual number of earthquakes for each magnitude bin. Note you can place all of the operations in one line of code inside the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preallocate the vector N with a zeros vector of size len(m)\n",
    "\n",
    "N = np.zeros(m.shape) # *****\n",
    "# Find N\n",
    "for i in range(m.size):\n",
    "    N[i] = np.count_nonzero(np.vectorize(lambda e: 1 if (e >= m[i] and (True if i == m.size - 1 else e < m[i+1])) else 0)(mag)) / (year[year.size-1] - year[0])\n",
    "print(N)\n",
    "\n",
    "# note that the final variable N should actually be log(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(m, N,'o') # ******\n",
    "plt.xlabel('mag')\n",
    "plt.ylabel('log(N)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data to find the Gutenberg Richter statistics.\n",
    "\n",
    "Now, fit the data with the Gutenberg Richter relationship $log_{10}(N(m))$=A+Bm. In other words, \"invert\" the data to find the applied model parameters. We will walk through the steps of this inversion.\n",
    "\n",
    "1) First, create the model parameter matrix, G, which has one column of 1's and a second column of magnitude bins, m.\n",
    "\n",
    "$$\n",
    "G=\\begin{pmatrix}\n",
    "1 &m_0\\\\\n",
    "1 &m_1\\\\\n",
    "1 &m_2\\\\\n",
    ". &.\\\\\n",
    ". &.\\\\\n",
    ". &.\\\\\n",
    "1 &m_n\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "2) Next, create the data matrix, d, which in this case is a single column and contains the $log_{10}(N(m))$ values. Note that we already defined this vector above.\n",
    "\n",
    "$$\n",
    "d=\\begin{pmatrix}\n",
    "log_{10}(N(m_0))\\\\\n",
    "log_{10}(N(m_1))\\\\\n",
    "log_{10}(N(m_2))\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "log_{10}(N(m_n))\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.ones(len(N))        # column of 1's\n",
    "G=np.column_stack((tmp,m)) # matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Next, compute the $G^{T}G$ matrix (G-transpose times G)  using numpy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTG=np.dot(np.transpose(G),G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Next, compute the $G^{T}D$ (A-transpose times D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = N # Recall that we have already defined the D-matrix above as \"N\"\n",
    "GTD=np.dot( , )   # *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Finally, solve the inverse problem. Invert the equation $(G^{T}G)x=G^{T}D$ using the numpy linear algebra solver (numpy.linalg.solv()). The result, x, will be a vector of the Gutenberg-Richter coefficients, in which the A-value is x[0] and the B-value is x[1]. The values you should get are A=3.418 and B=-0.809."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the linear inverse problem\n",
    "soln=np.linalg.solve( , ) # ******\n",
    "print(f'A-value={soln[0]:.3f}, B-value={soln[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear Gutenberg-Richter model is fully defined by the A and B coefficients, however in order to plot a line through the distribution we need to take the dot product of G with our two-parameter solution vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=m # the independent variable of the best-fit line is the same as for the data (magnitude bin)\n",
    "y=np.dot(G,soln) # synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector, \"y\", is called the synthetic data because it is a synthetic estimate of the real data. The difference between synthetic data and real data can be quantified through uncertainty analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty analysis of Gutenberg-Richter model\n",
    "\n",
    "Next, compute the uncertainties of the model (best-fit line defined by the A and B coefficients). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps outline how to compute 95% confidence intervals for the model using the numpy and scipy packages in Python.\n",
    "\n",
    "1) df=(length_of_data) - (number_of_model_parameters) #degree of freedom\n",
    "\n",
    "2) e=data-(model predictions) #prediction error\n",
    "\n",
    "3) variance=np.sum(e*e)/df\n",
    "\n",
    "4) se_y=np.sqrt(var)                       #standard error of the estimate\n",
    "\n",
    "5) sdev=np.sqrt(var)                       #standard deviation\n",
    "\n",
    "6) t=stats.t.ppf(1-0.05/2,degfree)             #two-sided students t-distribution\n",
    "\n",
    "7) lower95=np.exp(np.log(modeled_pga)-t*se_y)\n",
    "\n",
    "8) upper95=np.exp(np.log(modeled_pga)+t*se_y) \n",
    "\n",
    "9) se_b=sdev/np.sqrt(np.sum((x-np.mean(x))**2)) # standard error of slope\n",
    "\n",
    "10) se_a=sdev*np.sqrt(1/len(x) + np.mean(x)**2/np.sum((x-np.mean(x))**2)) # standard error of intercept (9 and 10 will be important for incorporating Gutenberg Richter uncertainty in PSHA (a future homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the uncertainty in Gutenberg-Richter Parameters\n",
    "\n",
    "length_of_data = len(N)\n",
    "number_of_model_parameters = 2\n",
    "df=(length_of_data) - (number_of_model_parameters) #degree of freedom\n",
    "\n",
    "e=N-y #prediction error\n",
    "\n",
    "var=np.sum(e*e)/df\n",
    "\n",
    "se_y=np.sqrt(var)             #standard error of the estimate\n",
    "sdev=np.sqrt(var)             #standard deviation\n",
    "\n",
    "#Calculate 95% confidence bounds\n",
    "t=stats.t.ppf(1-0.05/2,df)    #two-sided students t-distribution\n",
    "tmp=np.sqrt(1/len(x)+((x-np.mean(x))**2)/np.sum((x-np.mean(x))**2))\n",
    "tmp=tmp/max(tmp)\n",
    "lower95=y-t*se_y*tmp\n",
    "upper95=y+t*se_y*tmp\n",
    "se_b=sdev/np.sqrt(np.sum((x-np.mean(x))**2))                      #standard error slope\n",
    "se_a=sdev*np.sqrt(1/len(x) + np.mean(x)**2/np.sum((x-np.mean(x))**2)) #standard error of intercept\n",
    "a95=se_a*t\n",
    "b95=se_b*t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the fit to the data\n",
    "To visualize the fit to the distribution, we would like to make a plot showing the magnitude distribution as plotted above, log10(N) vs m, with the best fit line that we just found the coefficients for. Visalize the uncertainty by plotting the upper and lower 95 percentiles of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#plot the distribution like above\n",
    "ax.plot(m,N,'bo')\n",
    "\n",
    "#plot the best-fit line with uncertainties\n",
    "ax.plot(x,y,'r-',linewidth=2)\n",
    "\n",
    "# plot the 95% confidence intervals\n",
    "ax.plot( , ,'k-',linewidth=2) # *****\n",
    "ax.plot( , ,'k-',linewidth=2) # *****\n",
    "\n",
    "ax.set(xlabel='magnitude', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Initial Gutenberg-Richter Distribution')\n",
    "ax.grid()\n",
    "plt.savefig(\"hw1_ex3_figure2_withErrorBounds.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What do the Gutenberg Ricther statistics represent for the earthquake distribution?\n",
    "\n",
    "2. What happens to the shape of the distribution (log(N)) if you reduce the magnitude bin size by a factor of 10?\n",
    "2. Why does the model parameter matrix (G) have a column of 1's?\n",
    "3. What determines the size of $G^TG$ and $G^Td$?\n",
    "4. How well does the Gutenberg-Richter model fit the data? Quantify your answer in terms of uncertainty.\n",
    "5. Where does the fit begin to breakdown and why?\n",
    "6. Based on your Gutenberg-Richter coefficients what is the annual rates of a M4 earthquakes? For a M7 earthquake?\n",
    "7. On average how many years are there between M7 earthquakes based on this catalog.\n",
    "8. How many M7 earthquakes are in the catalog?\n",
    "9. What is your assessment of the quality or suitability of the forecast of average M7 occurrence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Declustering (30 pts)\n",
    "\n",
    "In the above analysis mainshocks (primary events) and aftershocks are mixed together. The results for the Gutenberg-Richter statistics were generally pretty good, however a correct implementation of Gutenberg-Richter considers only the primary events. Therefore, we seek a catalog with aftershocks removed in order to improve our assessment of the Gutenberg-Richter statistics. The process to remove aftershocks is called declustering.\n",
    "\n",
    "In this exercise, you will evaluate a published declustering method as you use it to decluster the catalog analyzed above. Then you will re-compute the Gutenberg-Richter coefficients for the declustered catalog in order to examine the affect on the G-R statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declustering Algorithm\n",
    "\n",
    "The analysis that was just performed was for the raw catalog, which means that it includes all events. However Gutenberg-Richter is really interested in the occurrence of primary \"main shock\" events, and therefore it is necessary to decluster the catalog to obtain an unbiased estimate of the G-N coefficients. Declustering here means remove the aftershocks from the catalog. This is done using an algorithm that relates the \"expected\" time and distance range of aftershocks from a given mainshock. Large mainshocks will result in aftershock populations that, statistically speaking, have a greater likelihood to occur over longer time periods and greater mainshock-aftershock distances compared with smaller mainshock-aftershock series. \n",
    "\n",
    "The code block below defines a declustering algorithm. This algorithm uses distance and time metrics that are magnitude dependent, called 'Dtest' and 'Ttest'. If a given event falls within the maximal values defined by Dtest and Ttest for its magnitude it is deemed an aftershock and removed from the catalog. After all events are processed, the remaining catalog is then comprised of only primary events. This declustered catalog can be used to estimate more accurate Gutenberg-Richter statistics. Furthermore, we can study the aftershock events that the algorithm removed for a given earthquake in the context of the Omori Law statistics (Exercise 4).\n",
    "\n",
    "Because aftershock identification is an empirical procedure, there are many different ways to define the Dtest and Ttest relationships. Stiphout et al., (2012, on page 10) summarizes three different definitions of the Dtest/Ttest relationships originally proposed by Uhrhammer (1986), Knopoff and Gardner (1972), and Gruenthal. \n",
    "\n",
    "Compare the event reduction rate (final number divided by the initial number of events) for the three different proposed distance and time windows. You can do this by adding a logical (if statement) tree to enable switching between different definitions of Dtest and Ttest in declustering_algorithm below. The first definition (Eqn 1 from Stiphout et al., 2012, p.10) has already been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declustering_algorithm(cat,definition=1):\n",
    "    '''\n",
    "    Decluster a catalog\n",
    "    \n",
    "    note: This function may take a few minutes to complete\n",
    "    \n",
    "    calls readAnssCatalog()\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    cat must be an anss formatted pandas datafram\n",
    "    definition is the algorithm (1 - 3) from Stiphout, 2012, which determines Dtest and Ttest values\n",
    "        Definition = 1 : Gardner and Knopoff, 1974 [default]\n",
    "        Definition = 2 : Gruenthal\n",
    "        Definition = 3 : Uhrhammer, 1986\n",
    "    \n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    # do not edit\n",
    "    cnt=0\n",
    "    save=np.zeros((1,10000000),dtype=int)\n",
    "\n",
    "    # grab catalog arrays\n",
    "    year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(cat)\n",
    "    ne=len(year)\n",
    "\n",
    "    # main for-loop over events\n",
    "    for i in range(0,ne,1):\n",
    "        \n",
    "        if definition == 1:\n",
    "            \n",
    "            # Definition #1 : Knopoff and Gardner, 1972\n",
    "            Dtest=np.power(10,0.1238*mag[i]+0.983)\n",
    "            if mag[i] >= 6.5:\n",
    "                Ttest=np.power(10,0.032*mag[i]+2.7389)\n",
    "            else:\n",
    "                Ttest=np.power(10,0.5409*mag[i]-0.547)\n",
    "\n",
    "        elif definition == 2:\n",
    "\n",
    "            # Definition #2 : Gruenthal # *****\n",
    "            Dtest=np.exp(1.77+(0.037+1.02*mag[i])**2)   # distance bounds\n",
    "            if mag[i] >= 6.5:\n",
    "                Ttest=abs(np.exp(-3.95+(0.62+17.32*mag[i])**2))  # aftershock time bounds for M >= 6.5\n",
    "            else:\n",
    "                Ttest=np.power(10,0.024*mag[i]+2.8)  # aftershock time bounds for M < 6.5\n",
    "    \n",
    "#         elif definition == 3:\n",
    "# \n",
    "            # Definition #3 # *****\n",
    "\n",
    "            \n",
    "            \n",
    "        a=days[i+1:ne]-days[i]\n",
    "        m=mag[i+1:ne]\n",
    "        b=haversine_np(lon[i],lat[i],lon[i+1:ne],lat[i+1:ne])\n",
    "\n",
    "        icnt=np.count_nonzero(a <= Ttest)\n",
    "        if icnt > 0:\n",
    "            itime=np.array(np.nonzero(a <= Ttest)) + (i+1)\n",
    "            for j in range(0,icnt,1):             \n",
    "                if b[j] <= Dtest and m[j] < mag[i]:\n",
    "                    save[0][cnt]=itime[0][j]\n",
    "                    cnt += 1 # save contains index of aftershocks in cat\n",
    "\n",
    "    #Note this is an array of indexes that will be used to delete events flagged \n",
    "                        #as aftershocks\n",
    "    save=np.delete(np.unique(save),0)  \n",
    "    \n",
    "    # Filter or slice out the declustered and aftershock dataframe catalogs from the \n",
    "    # original dataframe catalog \"data\" using \"save\" above.\n",
    "    cat_aftershocks = cat.iloc[np.unique(save)] # *****\n",
    "    cat_declustered = cat.iloc[~cat.index.isin(save)]\n",
    "    \n",
    "    cat_aftershocks.reset_index(drop=True, inplace=True)\n",
    "    cat_declustered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return cat_declustered, cat_aftershocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the declustering algorithm\n",
    "data_declustered, data_aftershocks = declustering_algorithm(data,definition=1)\n",
    "\n",
    "# This condition should print out \"True\" if the catalogs were separated correctly\n",
    "len(data) == len(data_aftershocks) + len(data_declustered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a map showing the declustered catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readAnssCatalog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-989c821962d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the declustered dataframe into numpy as an array with different d\"\" variable names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdyear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdday\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdhour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdmn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdsec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdlon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdmag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mddays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadAnssCatalog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_declustered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Make map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlat0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m36.75\u001b[0m \u001b[0;31m#Set Corners of Map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'readAnssCatalog' is not defined"
     ]
    }
   ],
   "source": [
    "# load the declustered dataframe into numpy as an array with different d\"\" variable names  \n",
    "dyear,dmonth,dday,dhour,dmn,dsec,dlat,dlon,dmag,ddays = readAnssCatalog(data_declustered)\n",
    "\n",
    "#Make map\n",
    "lat0=36.75 #Set Corners of Map\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "ydim=10      #height of plot\n",
    "xdim=ydim*(haversine_np(lon0,lat0,lon1,lat0)/haversine_np(lon0,lat0,lon0,lat1)) #scale width\n",
    "plt.figure(figsize=(ydim,xdim))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='longitude', ylabel='Latitude',\n",
    "       title='Declustered Catalog')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES,alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS)\n",
    "ax.add_feature(cfeature.STATES.with_scale('10m'))\n",
    "\n",
    "# Plot events as open circles with size and color proportional to event magnitude\n",
    "indx=np.argsort(dmag)   #determine sort index #Sort Descending to plot largest events on top\n",
    "x=dlon[indx]            #apply sort index\n",
    "y=dlat[indx]\n",
    "z=np.exp(dmag[indx])    #exponent to scale size\n",
    "c = plt.cm.plasma(z/max(z))\n",
    "plt.scatter(x, y, s=(z/2), facecolors='none', edgecolors=c, marker='o', linewidth=2, alpha=0.5) # *****\n",
    "\n",
    "# Add Berkeley, CA as a red square with size proportional to event magnitude\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=10) # *****\n",
    "\n",
    "#Save the plot by calling plt.savefig() BEFORE plt.show()\n",
    "plt.savefig('hw1_ex4_seismap_declust.pdf')\n",
    "plt.savefig('hw1_ex4_seismap_declust.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Re-compute the Gutenberg-Richter statistics as above for the declustered catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine and plot the Gutenberg-Richter Distribution for De-clustered data\n",
    "#You may want to adjust the magnitude range of the analysis to focus on where the catalog is complete\n",
    "m=np.arange(1.5,6.9,0.1)\n",
    "N=np.zeros(len(m))\n",
    "\n",
    "for i in range(0,len(m),1):\n",
    "    N[i]=np.log10(np.count_nonzero(dmag >= m[i])/numyr)\n",
    "\n",
    "#Invert for A and B values\n",
    "G=np.column_stack(( ,m)) # *****\n",
    "GTG=np.dot(np.transpose(G),G)\n",
    "GTD=np.dot(np.transpose(G),N)\n",
    "soln=np.linalg.solve( , ) # *****\n",
    "x=m\n",
    "y=np.dot( ,soln) # *****\n",
    "\n",
    "#Compute the uncertainty in Gutenberg-Richter Parameters\n",
    "df=len(N) - 2                 #degree of freedom\n",
    "e=N-y                         #prediction error\n",
    "var=np.sum(e**2)/df\n",
    "se_y=np.sqrt(var)             #standard error of the estimate\n",
    "sdev=np.sqrt(var)             #standard deviation\n",
    "\n",
    "#Calculate 95% confidence bounds\n",
    "t=stats.t.ppf(1-0.05/2,df)    #two-sided students t-distribution\n",
    "tmp=np.sqrt(1/len(x)+((x-np.mean(x))**2)/np.sum((x-np.mean(x))**2))\n",
    "tmp=tmp/max(tmp)\n",
    "lower95=y-t*se_y*tmp\n",
    "upper95=y+t*se_y*tmp\n",
    "se_b=sdev/np.sqrt(np.sum((x-np.mean(x))**2))                      #standard error slope\n",
    "se_a=sdev*np.sqrt(1/len(x) + np.mean(x)**2/np.sum((x-np.mean(x))**2)) #standard error of intercept\n",
    "a95=se_a*t\n",
    "b95=se_b*t\n",
    "\n",
    "#Now Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(m, N,'b.',x,y,'k-',x,lower95,'r-',x,upper95,'r-')\n",
    "ax.set(xlabel='magnitude', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Declustered Gutenberg-Richter Distribution')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"hw1_ex4_figure4.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f'A_value= {soln[0]:.3f} B_value={soln[1]:.3f}')\n",
    "print(f'95%intercept= {a95:.3f} 95%slope={b95:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. How many events were removed from the catalog by each declustering algorithm?\n",
    "\n",
    "2. Compare the spatial distribution of earthquakes between the raw and declustered catalogs.\n",
    "\n",
    "3. For the two other methods of declusting, how many events were removed from the catalog?\n",
    "\n",
    "4. Compare the Gutenberg-Richter A and B coefficients for the three versions of the declustered catalog.\n",
    "\n",
    "5. What is the annual rate of occurrence of M4 earthquakes for each of the declustered catalogs?\n",
    "\n",
    "6. What is the average M7 return period (inverse of annual occurrence of M7 events) for each of the declustered catalogs?\n",
    "\n",
    "7. Compare your estimated values with what has been presented in the USGS Earthquake Hazard Assessments of the return period for Hayward fault earthquakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Omori Law for Loma Prieta M6.9 Event (30 pts)\n",
    "\n",
    "Here we will use the declustering algorithm to identify aftershocks of the October 18 1989 at 04:15am (October 17 at 5:15pm PDT) the M6.9 Loma Prieta earthquake occurred in the Santa Cruz mountains approximately 80 km southwest of the Berkeley Campus. This wiki has some background information for the earthquake: https://en.wikipedia.org/wiki/1989_Loma_Prieta_earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Earthquake Catalog\n",
    "\n",
    "Load the .csv data file of all the earthquakes 1900 - 2018 in the ANSS (Advanced National Seismic System) catalog from 100 km around Berkeley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This catalog is a M0+ search centered at Berkeley radius=100km. \n",
    "# A big enough radius to include Loma Prieta but exclude Geysers.\n",
    "data=pd.read_csv('anss_catalog_1900to2018all.txt', sep=' ', delimiter=None, header=None,\n",
    "                 names = ['Year','Month','Day','Hour','Min','Sec','Lat','Lon','Mag'])\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select earthquakes related to the Loma Prieta Earthquake\n",
    "\n",
    "Use Boolean indexing to select events from the full catalog from between October 18, 1989 (date of mainshock) and December 18, 1989 (3-months following)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_1989 = data[(data.Year>=1) & (data.Year<1)]  # *****        #get one year of data\n",
    "\n",
    "fall_eq = EQ_1989[(EQ_1989.Month>9) & (EQ_1989.Month<=12)]    #collect months of Oct, Nov and Dec\n",
    "LP_eq = fall_eq[(~((fall_eq.Month==10) & (fall_eq.Day<18)))]  #negate events before day (assumes first month is 10)\n",
    "LP_eq = LP_eq[(~((LP_eq.Month==12) & (LP_eq.Day>18)))]        #negate events after day (assumes last month is 12)\n",
    "LP_eq.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data arrays for 3-month period beginning with Loma Prieta Earthquake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year,month,day,hour,minute,sec,lat,lon,mag = readAnssCatalog(EQ_1989)#override for plotting entire year catalog\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = readAnssCatalog(LP_eq)\n",
    "nevt = len(mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Loma Preita time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-e81dfd2ff7f1>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-e81dfd2ff7f1>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    ax.plot(  , ,'o',alpha=0.2,markersize=5) # *****\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# plot magnitude vs. day\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(  , ,'o',alpha=0.2,markersize=5) # *****\n",
    "ax.set(xlabel='Days', ylabel='Magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "ax.set_ylim([0,7])\n",
    "fig.savefig(\"hw1_ex4_ts_raw.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Number={nevt:d} MinMag={min(mag):.2f} MaxMag={max(mag):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Loma Preita Earthquake Catalog in map view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_aspect('auto')\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',\n",
    "       title='Earthquake Catalog')\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "indx=np.argsort(mag)   #determine sort index\n",
    "x=lon[indx]            #apply sort index\n",
    "y=lat[indx]\n",
    "z=np.exp(dmag[indx])    #exponent to scale size\n",
    "c = plt.cm.viridis_r(z/max(z)) # colormap scales with magnitude\n",
    "plt.scatter(x, y, s=(z), facecolors=c, alpha=0.5, edgecolors='k', marker='o', linewidth=2) # plot circles on EQs\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=8)  # plot red square on Berkeley\n",
    "\n",
    "plt.savefig(\"hw1_ex4_map_raw.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decluster the Raw Catalog for the Loma Prieta time period\n",
    "\n",
    "We use the same decluster algorithm previously to identify aftershocks and remove them from the 30-day Loma Preita catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dec, data_after = declustering_algorithm(LP_eq,definition=2)\n",
    "\n",
    "# This condition should print out \"True\" if the catalogs were separated correctly\n",
    "len(LP_eq) == len(data_after) + len(data_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two sets of arrays, one for the declustered catalog and one for the aftershock catalog. Use `np.delete()` to delete the aftershock events for the declustered catalog, and use `after` to select the aftershock events for the aftershock calalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyear,dmonth,dday,dhour,dminute,dsec,dlat,dlon,dmag,ddays = readAnssCatalog(data_dec)\n",
    "ayear,amonth,aday,ahour,aminute,asec,alat,alon,amag,adays = readAnssCatalog(data_after)\n",
    "dnevt =len(ddays)\n",
    "anevt=len(adays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Aftershock Catalog in time series\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot( ,  ,'o',alpha=0.2,markersize=5)\n",
    "ax.set(xlabel='days', ylabel='magnitude',\n",
    "       title='Aftershock Event Catalog')\n",
    "ax.grid()\n",
    "ax.set_ylim([0,7])\n",
    "fig.savefig(\"hw1_ex4_ts_aftershockOnly.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Number={anevt:d} MinMag={min(amag):.2f} MaxMag={max(amag):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of the declustered events\n",
    "\n",
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',\n",
    "       title='Aftershock Catalog')\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "indx=np.argsort(dmag)   #determine sort index\n",
    "x=dlon[indx]            #apply sort index\n",
    "y=dlat[indx]\n",
    "z=np.exp(dmag[indx])    #exponent to scale size\n",
    "c = plt.cm.viridis_r(z/max(z))\n",
    "plt.scatter(x, y, s=(z/2), facecolors=c, alpha=0.4, edgecolors='k', marker='o', linewidth=2)\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=8)\n",
    "\n",
    "plt.savefig(\"hw1_ex4_map_mainshockOnly.png\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of Aftershock events\n",
    "\n",
    "#Set Corners of Map\n",
    "lat0=36.75\n",
    "lat1=39.0\n",
    "lon0=-123.75\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',\n",
    "       title='Aftershock Catalog')\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "indx=np.argsort(amag)   #determine sort index\n",
    "x=alon[indx]            #apply sort index\n",
    "y=alat[indx]\n",
    "z=np.exp(amag[indx])    #exponent to scale size\n",
    "c = plt.cm.plasma(z/max(z))\n",
    "plt.scatter(x, y, s=(z/2), facecolors=c, alpha=0.4, edgecolors='k', marker='o', linewidth=2)\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=8)\n",
    "\n",
    "plt.savefig(\"hw1_ex4_map_aftershockOnly.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omori statistics\n",
    "\n",
    "To compute the Omori statistics we want to bin the log10 of the number of aftershocks each day following the mainshock and fit a power law equation such as:\n",
    "\n",
    "\\begin{matrix}\n",
    "N=\\frac{A}{(t+\\epsilon)^P}, \n",
    "\\end{matrix}\n",
    "\n",
    "where t is time in days, N is the number of earthquakes in the 24 hour period, and $\\epsilon$ is a small number (fraction of a day) to avoid the singularity at zero time. A and P are the coeffients that we want to find through regression. This power law equation can be linearized by simply taking the log10 of both sides giving:\n",
    "\n",
    "\\begin{matrix}\n",
    "log_{10}(N)=a - P*log_{10}(t+\\epsilon)\n",
    "\\end{matrix}\n",
    "\n",
    "Note: We will use both the Gutenberg-Richter and the Omori Law statistics computed in Homework 1 in Homework 2 where we will examine the probability of earthquake occurrence and aftershock occurrence following a given mainshock.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the number of aftershocks in each day\n",
    "epsilon=0.1\n",
    "maxdays=np.int(np.max(adays))\n",
    "t=np.arange(1,maxdays+2,1)\n",
    "logt=np.log10(t)\n",
    "N=np.zeros(maxdays+1)\n",
    "\n",
    "for i in range(0,maxdays+1,1):\n",
    "    N[i]=np.count_nonzero(adays == i)\n",
    "logN=np.log10(N)\n",
    "\n",
    "#Invert for A and B values\n",
    "G= # *****\n",
    "GTG=np.dot( , ) # *****\n",
    "GTD=np.dot( , ) # *****\n",
    "soln= # ***** \n",
    "x= # *****\n",
    "y= # *****\n",
    "\n",
    "print(f'A_value= {soln[0]:.3f} P_value={soln[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t-epsilon, N,'bo',x,y,'r-',linewidth=2)\n",
    "ax.set(xlabel='days after mainshock', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Omori Law')\n",
    "ax.grid()\n",
    "plt.savefig(\"hw1_ex4_OmoriStats_linlin.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.loglog(t-epsilon, N,'bo',x,y,'r-',linewidth=2)\n",
    "ax.set(xlabel='days after mainshock', ylabel='Number of Earthquakes (log10)',\n",
    "       title='Omori Law')\n",
    "ax.grid()\n",
    "plt.savefig(\"hw1_ex4_OmoriStats_loglog.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of the number of earthquakes per day for 1989\n",
    "\n",
    "#  create data arrays for the entirety of 1989\n",
    "year=EQ_1989.Year.values\n",
    "month=EQ_1989.Month.values\n",
    "day=EQ_1989.Day.values\n",
    "nevt=len(year)        #number of events \n",
    "\n",
    "#Determine the number of days from the first event\n",
    "days=np.zeros(nevt) # initialize the size of the array days\n",
    "\n",
    "for i in range(0,nevt,1):\n",
    "    d0 = datetime.date(year[0], month[0], day[0])\n",
    "    d1 = datetime.date(year[i], month[i], day[i])\n",
    "    delta = d1 - d0\n",
    "    days[i]=delta.days # fill days in with the number of days since the first event\n",
    "    \n",
    "maxdays=np.int(np.max(days))\n",
    "NN=np.zeros(maxdays+1)\n",
    "t=np.arange(0,maxdays+1,1)\n",
    "for i in range(0,maxdays+1,1):\n",
    "    NN[i]=np.count_nonzero(days == i)\n",
    "\n",
    "#Now Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, NN,'bo-',linewidth=2)\n",
    "ax.set(xlabel='day', ylabel='Number of Earthquakes',\n",
    "       title='Number of Earthquakes Per Day in 1989')\n",
    "ax.grid()\n",
    "plt.savefig(\"hw1_ex4_omori.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "2. Which faults were active during Loma Preita?\n",
    "\n",
    "3. What could cause aftershocks to occur on faults other than the mainshock fault?\n",
    "\n",
    "4. Here we used a 3-month period beginning at the Loma Prieta Earthquake. Examine the results taking a 6-month period beginning 3 months before the Loma Prieta earthquake. How does the distribution of earthquakes differ for the two time periods.\n",
    "\n",
    "1. How does the estimated P-value compare to values reported in Lay and Wallace?\n",
    "\n",
    "2. Is the aftershock more or less productive than average?\n",
    "\n",
    "3. What is the number of earthquakes per day in the region for the period leading up to the Loma Prieta earthquake?\n",
    "\n",
    "4. How long after the earthquake would does the applied Omori Law predict the that the aftershock rate falls to the pre-event rate of earthquakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
